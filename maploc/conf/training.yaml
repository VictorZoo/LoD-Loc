experiment:
  name: ???
  gpus: 1
  seed: 0
training:
  lr: 16e-4  
  lr_scheduler: 
    name: MultiStepLR
    args:
      milestones: [15, 25, 30, 35]
      gamma : 0.5
  finetune_from_checkpoint: null 
  trainer:
    val_check_interval: 1 
    log_every_n_steps: 200
    limit_val_batches: 1000
    max_steps: 500000 
    devices: ${experiment.gpus}
  checkpointing:
    monitor: "loss/total/val"
    save_top_k: 10
    mode: "min"
hydra:
  job:
    name: ${experiment.name}
    chdir: false
